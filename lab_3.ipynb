{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVsyGPllorS7",
        "outputId": "b3358fec-660d-4e64-86a5-5d3b41af24f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_OXP0PQpRxn"
      },
      "source": [
        "Switch to the project working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_ePmaRVViXr",
        "outputId": "3d0ef1a1-69fb-4a2b-8ffd-eaf97c4dc4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  2  969M    2 24.8M    0     0  3747k      0  0:04:24  0:00:06  0:04:18 5129k^C\n"
          ]
        }
      ],
      "source": [
        "!curl ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar -o inria.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km-iO7yBPvs1",
        "outputId": "5e3762c1-2d04-4946-b3a6-d25b7dbc9554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aciE5BXIP2J_",
        "outputId": "00b4523f-8b57-4df6-d636-f4f870aeda11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCJR5APSP9wr",
        "outputId": "fc07290e-471a-42a1-9bd6-e0aed4088910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 27 20:25:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb5172o9oys5",
        "outputId": "2d252a92-4c84-4724-e661-95f74dc25f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSE569S/lab3/adversarial-yolo\n",
            " adversarial_examples.ipynb   how_to_debug.md      predictions.jpg\n",
            " \u001b[0m\u001b[01;34mappliedpatches\u001b[0m/              image.py             \u001b[01;34m__pycache__\u001b[0m/\n",
            " batch_detect.py              \u001b[01;34minria\u001b[0m/               pytorch041_cuda92_colab.sh\n",
            " batch_rotate.py              inria.tar            README.md\n",
            " \u001b[01;34mcfg\u001b[0m/                         \u001b[01;34mlayers\u001b[0m/              recall.py\n",
            " cfg.py                       LICENSE              region_loss.py\n",
            " class_only.json              load_data.py         run_docker.sh\n",
            " class_shift.json             median_pool.py       \u001b[01;34mruns\u001b[0m/\n",
            " clean_results.json           \u001b[01;34mmodels\u001b[0m/              \u001b[01;34msaved_patches\u001b[0m/\n",
            " craft_adv.py                 noise_results.json   \u001b[01;34mscripts\u001b[0m/\n",
            " darknet.py                   \u001b[01;34mnon_printability\u001b[0m/    \u001b[01;34mtest\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/                       \u001b[01;34m'oude versies'\u001b[0m/       test_patch2.py\n",
            " dataset.py                   partial.py           test_patch3.py\n",
            " debug.py                     patch_config.py      test_patch.py\n",
            " demo.py                      \u001b[01;34mpatches\u001b[0m/             \u001b[01;34mtools\u001b[0m/\n",
            " detect2.py                   patch.jpg            train_patch.py\n",
            " detect.py                    patch_results.json   train.py\n",
            " \u001b[01;34mDGXContainer\u001b[0m/                patch_simen.json     up_results.json\n",
            " eval.py                      patch_up.json        utils2.py\n",
            " Evaluation.ipynb             \u001b[01;34mpersons\u001b[0m/             utils.py\n",
            " FocalLoss.py                 \u001b[01;34mpics\u001b[0m/                valid.py\n",
            " _gitignore                   pr-curve.eps         \u001b[01;34mweights\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CSE569S/lab3/adversarial-yolo\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ril7SrNXpiKC"
      },
      "source": [
        "Install packages that you need here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zola2_M0owGl",
        "outputId": "05b27678-7b35-4d25-bcda-a1881a4a0c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 40.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 43.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 26.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stWpTu-Zo90A",
        "outputId": "05b0affd-ab7e-4724-a991-cf5a0c67f942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kjSd0aliP1d8"
      },
      "outputs": [],
      "source": [
        "import tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyzJFl8ypqEp"
      },
      "source": [
        "## 2.1 Attack Human-Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX4vBER8p3RE"
      },
      "source": [
        "\n",
        "### (1) Detect without patch\n",
        "Utilize the apis in detect.py to recognize images of people. Feel free to edit the source code if it’s needed. The result should be plotted after running the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtHxSlg2qT_F",
        "outputId": "f7e34b18-27eb-4a8d-fc93-bee03f12b9f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:File `'file_name.py'` not found.\n"
          ]
        }
      ],
      "source": [
        "# Your code starts here\n",
        "# Hint: To directly run the main function in a file, use \n",
        "%run file_name.py param1 param2 ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAFQsFiohUgm"
      },
      "outputs": [],
      "source": [
        "%run train_patch.py 'paper_obj'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCflo3z5brF3",
        "outputId": "5c85af03-f9dd-4efa-a8ec-3f2a37c20f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer     filters    size              input                output\n",
            "    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32\n",
            "    1 max          2 x 2 / 2   608 x 608 x  32   ->   304 x 304 x  32\n",
            "    2 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64\n",
            "    3 max          2 x 2 / 2   304 x 304 x  64   ->   152 x 152 x  64\n",
            "    4 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128\n",
            "    5 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64\n",
            "    6 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128\n",
            "    7 max          2 x 2 / 2   152 x 152 x 128   ->    76 x  76 x 128\n",
            "    8 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256\n",
            "    9 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128\n",
            "   10 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256\n",
            "   11 max          2 x 2 / 2    76 x  76 x 256   ->    38 x  38 x 256\n",
            "   12 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   13 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256\n",
            "   14 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   15 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256\n",
            "   16 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   17 max          2 x 2 / 2    38 x  38 x 512   ->    19 x  19 x 512\n",
            "   18 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   19 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512\n",
            "   20 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   21 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512\n",
            "   22 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   23 conv   1024  3 x 3 / 1    19 x  19 x1024   ->    19 x  19 x1024\n",
            "   24 conv   1024  3 x 3 / 1    19 x  19 x1024   ->    19 x  19 x1024\n",
            "   25 route  16\n",
            "   26 conv     64  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x  64\n",
            "   27 reorg              / 2    38 x  38 x  64   ->    19 x  19 x 256\n",
            "   28 route  27 24\n",
            "   29 conv   1024  3 x 3 / 1    19 x  19 x1280   ->    19 x  19 x1024\n",
            "   30 conv    425  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 425\n",
            "   31 detection\n",
            "Loading weights from weights/yolov2.weights... Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/CSE569S/lab3/adversarial-yolo/utils.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1805, 80])\n",
            "persons/person_079.jpg: Predicted in 0.625070 seconds.\n",
            "[0]person: 0.995249\n",
            "[39]bottle: 0.984207\n",
            "[56]chair: 0.990320\n",
            "[73]book: 0.978198\n",
            "[73]book: 0.998481\n",
            "save plot results to predictions.jpg\n"
          ]
        }
      ],
      "source": [
        "%run detect.py 'cfg/yolov2.cfg' 'weights/yolov2.weights' 'persons/person_079.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E31o5roroyh"
      },
      "source": [
        "### (2) Train patches\n",
        "Now we will train two patches by ourselves. One of the patches has a starter image as input and the other one is completely generated by our code. To do these, please \n",
        "\n",
        "\n",
        "*   Take a look at train_patch.py and edit the relevant code.\n",
        "*   Write a wrapped-up api for generating these two kinds of patches.\n",
        "*   Call the api to generate a with-starter-image patch and a without-starter-image patch.\n",
        "\n",
        "Please make sure to save these images since you need to use them in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjXyqjr_s9ul"
      },
      "outputs": [],
      "source": [
        "# starter_img can be an img path or the string 'no'\n",
        "def patch_training_caller(starter_img):\n",
        "  %run train_patch.py 'paper_obj' $starter_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl-Tqv0btOOb"
      },
      "outputs": [],
      "source": [
        "# Call patch_training_caller to generate a patch with starter image\n",
        "# Your code starts here\n",
        "patch_training_caller('saved_patches/patchnew0.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLZyBqf5tORs"
      },
      "outputs": [],
      "source": [
        "# Call patch_training_caller to generate a patch without starter image\n",
        "# Your code starts here\n",
        "patch_training_caller('no')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fKx3bN5uPTh"
      },
      "source": [
        "### (3) Detect people with patches\n",
        "Use the same api in detect.py as section 2.1 (1) to detect people who hold the patch picture you generated in 2.1 (2). The model should not recognize the people in this case. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYbls7kEugLg"
      },
      "outputs": [],
      "source": [
        "# Your code starts here\n",
        "# We did this during class time.  By holding our trained patch in front of ourselves, we were able to fool the object detector into thinking we weren't there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcL-eMMzurSX"
      },
      "source": [
        "## 2.2 Attack Stop Signs-Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFhNYzc-uuAI"
      },
      "source": [
        "### (1) Detect without patch\n",
        "Similar to 2.1 (1), first we need to use apis in detect.py to recognize images of stop signs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOZ7-kthuzu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14162d9-d1a8-4457-e98b-6ad12ef29108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer     filters    size              input                output\n",
            "    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32\n",
            "    1 max          2 x 2 / 2   608 x 608 x  32   ->   304 x 304 x  32\n",
            "    2 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64\n",
            "    3 max          2 x 2 / 2   304 x 304 x  64   ->   152 x 152 x  64\n",
            "    4 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128\n",
            "    5 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64\n",
            "    6 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128\n",
            "    7 max          2 x 2 / 2   152 x 152 x 128   ->    76 x  76 x 128\n",
            "    8 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256\n",
            "    9 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128\n",
            "   10 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256\n",
            "   11 max          2 x 2 / 2    76 x  76 x 256   ->    38 x  38 x 256\n",
            "   12 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   13 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256\n",
            "   14 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   15 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256\n",
            "   16 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512\n",
            "   17 max          2 x 2 / 2    38 x  38 x 512   ->    19 x  19 x 512\n",
            "   18 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   19 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512\n",
            "   20 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   21 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512\n",
            "   22 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024\n",
            "   23 conv   1024  3 x 3 / 1    19 x  19 x1024   ->    19 x  19 x1024\n",
            "   24 conv   1024  3 x 3 / 1    19 x  19 x1024   ->    19 x  19 x1024\n",
            "   25 route  16\n",
            "   26 conv     64  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x  64\n",
            "   27 reorg              / 2    38 x  38 x  64   ->    19 x  19 x 256\n",
            "   28 route  27 24\n",
            "   29 conv   1024  3 x 3 / 1    19 x  19 x1280   ->    19 x  19 x1024\n",
            "   30 conv    425  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 425\n",
            "   31 detection\n",
            "Loading weights from weights/yolov2.weights... Done!\n",
            "torch.Size([1805, 80])\n",
            "data/stop.jpg: Predicted in 0.043245 seconds.\n",
            "[11]stop sign: 0.999393\n",
            "save plot results to predictions.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/CSE569S/lab3/adversarial-yolo/utils.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
          ]
        }
      ],
      "source": [
        "# Your code starts here\n",
        "%run detect.py 'cfg/yolov2.cfg' 'weights/yolov2.weights' 'data/stop.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFzefl4u1Ri"
      },
      "source": [
        "## (2) Patch stop signs\n",
        "* Edit the source code involved in the patch-sticking control flow to attach the patch image on all the stop sign images. Images belonging to any other class (i.e. people, horses, etc.) should not be patched.\n",
        "* Write a wrapped-up api for sticking patches on stop signs.\n",
        "* Call the api to stick patch on stop sign. You should specify which patch is used in parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "11ERnO-wDZi9"
      },
      "outputs": [],
      "source": [
        "# This function does not take in parameters since test_patch itself was modified to take in the images\n",
        "def patch_sticking_caller():\n",
        "  # If there's no stop sign in the image, the image should not be patched.\n",
        "  # Your code starts here\n",
        "  %run test_patch3.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_sticking_caller()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEhbMRLos_di",
        "outputId": "af219836-f1fd-4bd2-a1e3-5bd2c18d3c28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting test read\n",
            "img read!\n",
            "starting test read\n",
            "img read!\n",
            "Setting everything up\n",
            "PatchTransformer(\n",
            "  (medianpooler): MedianPool2d()\n",
            ")\n",
            "Done\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/CSE569S/lab3/adversarial-yolo/utils.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1805, 80])\n",
            "before transform\n",
            "in patch transform\n",
            "skipped all noise shit\n",
            "AAAAAAAAAAAAAAAAAAAAA\n",
            "after transform\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4256: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4194: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1805, 80])\n",
            "new image\n",
            "new image\n",
            "new image\n",
            "new image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYOEy7Ce0A0d"
      },
      "source": [
        "### (3) Detect stop sign with patch\n",
        "Use the same api in detect.py as section 2.2 (1) to detect the patched stop sign. The model should not recognize the stop sign in this case. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScyH85Oo0FF0"
      },
      "outputs": [],
      "source": [
        "# Your code starts here\n",
        "# We did this part in class as well.  Using our trained patch we were able to fool the object detection network and make the car run over the stop sign-="
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7E31o5roroyh"
      ],
      "name": "lab3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}